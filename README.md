# NordicDataFlow ğŸŒŠ

A real-time data pipeline for Nordic economic and company data, demonstrating core data engineering skills with Azure cloud services.

[![Azure Static Web App](https://img.shields.io/badge/Azure-Dashboard-0078D4?logo=azure)](https://salmon-glacier-0eba51103.4.azurestaticapps.net)

## ğŸ—ï¸ Architecture (Medallion)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     INGEST      â”‚â”€â”€â”€â–¶â”‚    TRANSFORM    â”‚â”€â”€â”€â–¶â”‚      LOAD       â”‚â”€â”€â”€â–¶â”‚    CONSUME      â”‚
â”‚    (Bronze)     â”‚    â”‚    (Silver)     â”‚    â”‚     (Gold)      â”‚    â”‚   (Dashboard)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚                      â”‚                      â”‚
   Raw JSON              Cleaned Parquet        Azure SQL DB           React App
  Azure Blob               Azure Blob           Star Schema            (Planned)
```

### Data Layers

| Layer | Storage | Format | Purpose |
|-------|---------|--------|---------|
| **Bronze** | Azure Blob (`bronze/`) | JSON | Raw API data, unchanged |
| **Silver** | Azure Blob (`silver/`) | Parquet | Cleaned, validated, typed |
| **Gold** | Azure SQL Database | Tables | Aggregated, query-optimized |

## ğŸ“Š Data Sources

| Source | API | Data Type |
|--------|-----|-----------|
| **Statistics Finland** | StatFi PxWeb | Economic indicators |
| **PRH (YTJ)** | Business Register | Finnish company data |
| **Eurostat** | JSON API | EU GDP & statistics |
| **Fingrid** | Open Data API | Real-time electricity production |

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Language** | Python 3.10+ | ETL processing |
| **Storage** | Azure Blob Storage | Data lake (Bronze/Silver) |
| **Database** | Azure SQL (Serverless) | Gold layer warehouse |
| **ETL** | Pandas, PyODBC, PyArrow | Data transformation |
| **Orchestration** | Python Scripts / GitHub Actions | Pipeline scheduling |

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+ (or UV package manager)
- Azure CLI (signed in)
- ODBC Driver 17 for SQL Server

### Installation (with UV - Recommended)

[UV](https://github.com/astral-sh/uv) is an extremely fast Python package manager.

```bash
# Clone the repository
git clone https://github.com/yourusername/NordicDataPipeline.git
cd NordicDataPipeline

# Create virtual environment with Python 3.11
uv venv --python 3.11

# Activate (Windows PowerShell)
.\.venv\Scripts\activate

# Install dependencies (fast!)
uv pip install -r requirements.txt
```

### Installation (Traditional pip)

```bash
# Create virtual environment
python -m venv .venv
.\.venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create a `.env` file (see `.env.example`):

```env
AZURE_STORAGE_CONNECTION_STRING="<your-connection-string>"
SQL_SERVER="nordicdataflow-sql-3288.database.windows.net"
SQL_DATABASE="NordicDataDB"
SQL_USER="sqladmin"
SQL_PASSWORD="<your-password>"
FINGRID_API_KEY="<your-api-key>"
```

### Running the Pipeline

```bash
# Initialize database schema (first time only)
python -m src.pipeline setup

# Run full ETL pipeline
python -m src.pipeline

# Run individual phases
python -m src.ingest      # APIs â†’ Bronze
python -m src.transform   # Bronze â†’ Silver
python -m src.database    # Silver â†’ Gold

# Test API connectivity
python test_apis.py
```

## ğŸ“ Project Structure

```
NordicDataPipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py       # Configuration & env vars
â”‚   â”œâ”€â”€ storage.py      # Azure Blob Storage client
â”‚   â”œâ”€â”€ ingest.py       # Data ingestion (Bronze)
â”‚   â”œâ”€â”€ transform.py    # Data transformation (Silver)
â”‚   â”œâ”€â”€ database.py     # SQL Database operations (Gold)
â”‚   â””â”€â”€ pipeline.py     # Main orchestrator
â”œâ”€â”€ test_apis.py        # API connectivity tests
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env                # Environment variables (not in git)
â”œâ”€â”€ SETUP.md            # Infrastructure setup guide
â””â”€â”€ README.md           # This file
```

## ğŸ“ Database Schema (Gold Layer)

```sql
-- Dimension: Companies
dim_companies (
    company_id, business_id, name, registration_date,
    company_form, status, city, post_code, loaded_at
)

-- Fact: Electricity Production
fact_electricity_production (
    record_id, start_time, end_time, value_mw,
    dataset_id, hour_of_day, day_of_week, date_key, loaded_at
)

-- Dimension: Statistics Categories
dim_stat_categories (
    category_id, external_id, name, category_type,
    last_updated, loaded_at
)

-- Logging: Pipeline Runs
pipeline_runs (
    run_id, run_timestamp, source_name, records_processed,
    status, error_message
)
```

## ğŸ”§ Azure Resources

| Resource | Name | Purpose |
|----------|------|---------|
| **Resource Group** | `NordicDataFlow-RG` | Container for all resources |
| **Storage Account** | `nordicdataflow6121` | Data lake storage |
| **SQL Server** | `nordicdataflow-sql-3288` | Database server |
| **SQL Database** | `NordicDataDB` | Gold layer warehouse |

## ğŸ“Š Pipeline Output Example

```
============================================================
ğŸŒŠ NordicDataFlow Pipeline - Starting
â° Run Time: 2026-01-18T20:58:01
============================================================

ğŸ“¥ PHASE 1: INGESTION (Bronze Layer)
   âœ… StatFi: 149 categories
   âœ… PRH (Vivicta): 0 companies
   âœ… Eurostat: GDP data
   âœ… Fingrid: 10 records

ğŸ”„ PHASE 2: TRANSFORMATION (Silver Layer)
   âœ… Transformed 10 electricity records
   âœ… Transformed 149 categories

ğŸ“¤ PHASE 3: LOADING (Gold Layer)
   âœ… Loaded 20 electricity records to Gold

============================================================
ğŸ Pipeline Complete!
============================================================
```

## ğŸ“š Skills Demonstrated

- âœ… **Python** - ETL processing, API integration
- âœ… **SQL** - Schema design, query optimization
- âœ… **Azure** - Blob Storage, SQL Database, CLI
- âœ… **ETL/ELT Pipelines** - Medallion architecture
- âœ… **Data Modeling** - Star schema (fact/dimension)
- âœ… **DevOps** - Git, environment management

## ğŸ“„ License

MIT License - Built for Vivicta Data Engineer application.
